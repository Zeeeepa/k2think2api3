# K2Think API Proxy - Complete Documentation

**ğŸš€ OpenAI-compatible API proxy for MBZUAI K2-Think model | Built with FastAPI**

---

## ğŸ”“ Open Proxy Mode (NEW!)

The proxy now operates in **open mode** by default - the most flexible way to use K2-Think!

### What This Means

- âœ… **Any API Key Accepted**: Use `sk-any`, `test`, `random-key`, or even empty strings
- âœ… **Any Model Name Accepted**: `gpt-4`, `gpt-5`, `claude-3`, `my-model` - all route to K2-Think
- âœ… **Drop-in OpenAI Replacement**: Perfect replacement for OpenAI API endpoints
- âœ… **Zero Configuration**: No need to manage API keys or model names
- âœ… **Centralized Auth**: Server handles all K2Think authentication

### Quick Example

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-any",  # âœ… Any key works!
    base_url="http://localhost:7000/v1"
)

result = client.chat.completions.create(
    model="gpt-5",  # âœ… Any model works!
    messages=[{"role": "user", "content": "Write a haiku about code."}]
)

print(result.choices[0].message.content)
```

**Output:**
```
Code lines softly hum,  
Syntax weaves logic's soft songâ€”  
Machines come alive.
```

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Open Proxy Mode Details](#open-proxy-mode-details)
3. [Installation Methods](#installation-methods)
4. [Core Features](#core-features)
5. [Project Structure](#project-structure)
6. [Configuration](#configuration)
7. [Using the API](#using-the-api)
8. [Server Management](#server-management)
9. [API Reference](#api-reference)
10. [Troubleshooting](#troubleshooting)
11. [Advanced Features](#advanced-features)
12. [Changelog](#changelog)
13. [ä¸­æ–‡æ–‡æ¡£](#ä¸­æ–‡æ–‡æ¡£)

---

## Quick Start

### âš¡ One-Command Installation (Recommended)

The fastest way to get started - everything is automated:

```bash
bash <(curl -fsSL https://raw.githubusercontent.com/Zeeeepa/k2think2api3/main/install.sh)
```

**What happens automatically:**
1. âœ… Asks for your K2 credentials
2. âœ… Clones the repository
3. âœ… Installs dependencies
4. âœ… Starts the server on port 7000
5. âœ… Runs a test request
6. âœ… Displays usage instructions

### With Pre-Set Credentials (No Prompts)

```bash
export K2_EMAIL="your@email.com"
export K2_PASSWORD="yourpassword"
bash <(curl -fsSL https://raw.githubusercontent.com/Zeeeepa/k2think2api3/main/install.sh)
```

---

## Open Proxy Mode Details

### How It Works

1. **Client Request**: Client sends request with any API key + any model name
2. **Key Bypass**: Server accepts the key without validation
3. **Model Mapping**: Server maps client's model name â†’ `MBZUAI-IFM/K2-Think`
4. **Token Injection**: Server uses its own K2Think JWT token
5. **Upstream Request**: Server forwards to K2Think backend
6. **Response**: Server returns K2Think response to client

### Token Management

The server automatically manages K2Think JWT tokens:

- **Stored in**: `data/tokens.txt`
- **Credentials from**: `data/accounts.txt`
- **Auto-refresh**: When tokens expire (401 errors)
- **Zero-downtime**: Token rotation without service interruption

### Testing Different Configurations

```python
# Test 1: Different API keys
for api_key in ["sk-test-1", "random-key", "any-string", ""]:
    client = OpenAI(api_key=api_key or "empty", base_url="http://localhost:7000/v1")
    result = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Hi"}]
    )
    print(f"âœ… API key '{api_key or '(empty)'}' accepted")

# Test 2: Different model names
for model in ["gpt-4", "gpt-5", "claude-3", "my-model"]:
    client = OpenAI(api_key="test", base_url="http://localhost:7000/v1")
    result = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": "Hi"}]
    )
    print(f"âœ… Model '{model}' accepted â†’ routed to K2-Think")
```

### Benefits

**For Users:**
- ğŸ‰ **Zero Configuration** - No need to manage API keys
- ğŸ”„ **Model Flexibility** - Use any model name they prefer
- ğŸš€ **Instant Setup** - One command to get started
- ğŸ”’ **Security** - Server handles all authentication

**For Developers:**
- ğŸ“¦ **Easy Integration** - Works with existing OpenAI clients
- ğŸ”§ **No Code Changes** - Drop-in replacement for OpenAI API
- ğŸ§ª **Testing Friendly** - Use any mock keys for testing
- ğŸ“Š **Centralized Auth** - All tokens managed server-side

### Security Considerations

âš ï¸ **Important**: Since API key validation is bypassed:

- âœ… Deploy in trusted network environment
- âœ… Configure network-level security (firewall, VPN)
- âœ… Restrict access to authorized users/networks
- âœ… Secure the server's `data/tokens.txt` and `data/accounts.txt`
- âœ… Use environment variables for sensitive data

---

## Installation Methods

### Method 1: Bash Process Substitution (Recommended)

**Most foolproof method - prevents URL mistakes:**

```bash
bash <(curl -fsSL https://raw.githubusercontent.com/Zeeeepa/k2think2api3/main/install.sh)
```

**Why this is better:**
- ğŸ”’ Bash process substitution `<()` prevents file system errors
- ğŸ¯ Single command = less room for mistakes
- ğŸš€ Cleaner, more professional
- âœ… Industry-standard pattern (used by Homebrew, rustup, nvm)

### Method 2: Download and Inspect First

If you want to review the script before running:

```bash
# Download
curl -fsSL https://raw.githubusercontent.com/Zeeeepa/k2think2api3/main/install.sh -o install.sh

# Review (optional)
cat install.sh

# Run
bash install.sh
```

### Method 3: Manual Step-by-Step

For those who prefer manual control:

```bash
# 1. Clone repository
git clone https://github.com/Zeeeepa/k2think2api3.git
cd k2think2api3

# 2. Set credentials
export K2_EMAIL="your@email.com"
export K2_PASSWORD="yourpassword"

# 3. Setup environment
bash scripts/setup.sh

# 4. Start server
bash scripts/start.sh

# 5. Test API
bash scripts/send_request.sh
```

---

## Core Features

### ğŸ§  K2-Think Model Integration
- Full support for MBZUAI's K2-Think reasoning model
- Advanced AI capabilities with reasoning chains
- Optimized for complex problem-solving

### ğŸ”„ OpenAI Compatibility
- **Drop-in replacement** for OpenAI API
- Compatible with all OpenAI SDKs and libraries
- Same API format and behavior
- Seamless migration from OpenAI

### âš¡ Streaming Support
- Real-time streaming chat responses
- Efficient token-by-token output
- Low latency for better user experience

### ğŸ› ï¸ Function Calling
- Full OpenAI Function Calling support
- Tool use capabilities
- Structured data extraction
- Agent-ready architecture

### ğŸ“Š File Upload Support
- Image upload and analysis
- Document processing
- Multi-modal capabilities

### ğŸ” Intelligent Token Management
- **Multi-token rotation** with automatic failover
- Smart failure detection
- Auto-refresh on consecutive failures
- Zero-downtime token updates
- Continuous health monitoring

---

## Project Structure

```
k2think2api3/
â”œâ”€â”€ ğŸ“‚ src/                  # Source code
â”‚   â”œâ”€â”€ api_handler.py       # API request handling (Open Proxy Mode!)
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ token_manager.py     # Token rotation and management
â”‚   â””â”€â”€ token_updater.py     # Automatic token refresh
â”œâ”€â”€ ğŸ“‚ data/                 # Runtime data
â”‚   â”œâ”€â”€ accounts.txt         # K2 credentials (JSON format)
â”‚   â””â”€â”€ tokens.txt           # Auto-refreshing K2Think JWT tokens
â”œâ”€â”€ ğŸ“‚ scripts/              # Deployment scripts
â”‚   â”œâ”€â”€ all.sh               # ğŸ¯ Complete one-command setup
â”‚   â”œâ”€â”€ setup.sh             # Environment setup
â”‚   â”œâ”€â”€ start.sh             # Server startup
â”‚   â””â”€â”€ send_request.sh      # API testing
â”œâ”€â”€ k2think_proxy.py         # Main server entry point
â”œâ”€â”€ get_tokens.py            # Token acquisition utility
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env                     # Configuration (auto-generated)
â”œâ”€â”€ server.log               # Server logs
â””â”€â”€ README.md                # This file
```

---

## Configuration

### Environment Variables (.env)

The `.env` file is auto-generated during setup. Key settings:

```bash
# Server Configuration
PORT=7000
HOST=0.0.0.0

# Token Auto-Update (requires accounts.txt)
ENABLE_TOKEN_AUTO_UPDATE=true
TOKEN_UPDATE_INTERVAL=3600  # 1 hour in seconds

# Debug Mode
DEBUG_LOGGING=false
```

### Accounts Configuration (data/accounts.txt)

JSON format for K2 credentials:

```json
{"email": "your@email.com", "k2_password": "yourpassword"}
```

**Security Notes:**
- Never commit this file to version control
- Keep file permissions restrictive (chmod 600)
- Use environment variables when possible

---

## Using the API

### Python (OpenAI SDK)

```python
from openai import OpenAI

# Initialize client - any API key works!
client = OpenAI(
    api_key="sk-any",  # Can be anything
    base_url="http://localhost:7000/v1"
)

# Simple request - any model name works!
response = client.chat.completions.create(
    model="gpt-4",  # Will route to K2-Think
    messages=[
        {"role": "user", "content": "Explain quantum computing"}
    ]
)

print(response.choices[0].message.content)

# Streaming request
stream = client.chat.completions.create(
    model="gpt-5",  # Will route to K2-Think
    messages=[
        {"role": "user", "content": "Write a story"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

# Function calling
response = client.chat.completions.create(
    model="claude-3",  # Will route to K2-Think
    messages=[{"role": "user", "content": "What's the weather?"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather information",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                }
            }
        }
    }]
)
```

### cURL

```bash
# Simple request
curl http://localhost:7000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-any" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# Streaming request
curl http://localhost:7000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer test-key" \
  -d '{
    "model": "gpt-5",
    "messages": [{"role": "user", "content": "Tell me a joke"}],
    "stream": true
  }'
```

### JavaScript/TypeScript

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: 'sk-any',  // Any key works
  baseURL: 'http://localhost:7000/v1'
});

async function main() {
  const response = await client.chat.completions.create({
    model: 'gpt-4',  // Will route to K2-Think
    messages: [{ role: 'user', content: 'Hello!' }]
  });
  
  console.log(response.choices[0].message.content);
}

main();
```

---

## Server Management

### Start Server

```bash
cd k2think2api3
bash scripts/start.sh
```

### Stop Server

```bash
# Using PID file
kill $(cat .server.pid)

# Or find and kill
pkill -f k2think_proxy
```

### Restart Server

```bash
cd k2think2api3
kill $(cat .server.pid)
bash scripts/start.sh
```

### View Logs

```bash
# Real-time logs
tail -f server.log

# Recent logs
tail -n 100 server.log

# Search logs
grep "error" server.log
```

### Check Server Status

```bash
# Health check
curl http://localhost:7000/health

# Token statistics
curl http://localhost:7000/admin/tokens/stats

# Server process
ps aux | grep k2think_proxy
```

---

## API Reference

### Endpoints

#### POST /v1/chat/completions

Create a chat completion (OpenAI compatible).

**Request:**
```json
{
  "model": "gpt-4",  // Any model name
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "stream": false,
  "temperature": 0.7,
  "max_tokens": null
}
```

**Response:**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "MBZUAI-IFM/K2-Think",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Hello! How can I help you?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

#### GET /v1/models

List available models.

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "MBZUAI-IFM/K2-Think",
      "object": "model",
      "created": 1686935002,
      "owned_by": "MBZUAI"
    }
  ]
}
```

#### GET /health

Server health check.

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:00:00Z"
}
```

#### GET /admin/tokens/stats

Token pool statistics.

**Response:**
```json
{
  "total_tokens": 5,
  "available_tokens": 4,
  "failed_tokens": 1,
  "token_details": [...]
}
```

---

## Troubleshooting

### Server Won't Start

**Check if port 7000 is in use:**
```bash
lsof -i :7000
```

**View server logs:**
```bash
cat server.log
```

**Verify Python version:**
```bash
python3 --version  # Should be 3.8+
```

### "Module not found: openai"

**Use virtual environment:**
```bash
source venv/bin/activate
python your_script.py
```

**Or use venv Python directly:**
```bash
./venv/bin/python your_script.py
```

### Token Auto-Update Not Working

1. **Verify accounts.txt exists:**
   ```bash
   cat data/accounts.txt
   ```

2. **Check .env configuration:**
   ```bash
   grep ENABLE_TOKEN_AUTO_UPDATE .env
   ```

3. **View updater logs:**
   ```bash
   grep "token" server.log
   ```

### Connection Refused

**Ensure server is running:**
```bash
ps aux | grep k2think_proxy
```

**Test health endpoint:**
```bash
curl http://localhost:7000/health
```

---

## Advanced Features

### Custom Port Configuration

Edit `.env`:
```bash
PORT=8000  # Change from default 7000
```

Restart server:
```bash
kill $(cat .server.pid)
bash scripts/start.sh
```

### Production Deployment

1. **Use reverse proxy** (nginx, Apache)
2. **Set up SSL/TLS** for secure connections
3. **Configure rate limiting**
4. **Set up monitoring** and alerts
5. **Use process manager** (systemd, PM2)
6. **Implement log rotation**
7. **Set up automated backups**

### Docker Deployment

```bash
# Build image
docker build -t k2think-api .

# Run container
docker run -d \
  -p 7000:7000 \
  -e K2_EMAIL="your@email.com" \
  -e K2_PASSWORD="yourpassword" \
  --name k2think-api \
  k2think-api
```

---

## Changelog

### Version 2.1.0 - Open Proxy Mode (2025-01-15)

**Major Changes:**

1. **Open Proxy Mode Enabled**
   - âœ… Accept any API key from clients
   - âœ… Accept any model name (auto-route to K2-Think)
   - âœ… Bypass client key validation
   - âœ… Use server-managed K2Think tokens

2. **Code Changes**
   - Modified `src/api_handler.py`:
     - `validate_api_key()` now always returns `True`
     - `get_actual_model_id()` always returns K2-Think model
   - Modified `src/config.py`:
     - Made `VALID_API_KEY` optional
   - Enhanced token management and auto-refresh

3. **Documentation**
   - Added Open Proxy Mode section
   - Updated all examples to show flexibility
   - Added security considerations
   - Consolidated documentation

**Migration Guide:**

No changes needed for existing users! The system is fully backward compatible.

**Benefits:**
- Drop-in replacement for OpenAI API
- Simplified client configuration
- Zero-configuration testing
- Centralized authentication

---

## ä¸­æ–‡æ–‡æ¡£

<details>
<summary>ç‚¹å‡»å±•å¼€å®Œæ•´ä¸­æ–‡æ–‡æ¡£ / Click to expand full Chinese documentation</summary>

### åŸºäº FastAPI æ„å»ºçš„ K2Think AI æ¨¡å‹ä»£ç†æœåŠ¡

æä¾› OpenAI å…¼å®¹çš„ API æ¥å£ï¼Œæ”¯æŒæœ¬åœ°å’Œ Docker éƒ¨ç½²ã€‚

### ğŸ”“ å¼€æ”¾ä»£ç†æ¨¡å¼ï¼ˆæ–°åŠŸèƒ½ï¼ï¼‰

ä»£ç†æœåŠ¡å™¨ç°åœ¨é»˜è®¤ä»¥**å¼€æ”¾æ¨¡å¼**è¿è¡Œï¼š

- âœ… **æ¥å—ä»»ä½• API å¯†é’¥**ï¼šå¯ä»¥ä½¿ç”¨ä»»ä½•å¯†é’¥ï¼ˆå¦‚ `sk-any`ã€`test` ç­‰ï¼‰
- âœ… **æ¥å—ä»»ä½•æ¨¡å‹åç§°**ï¼šæ‰€æœ‰æ¨¡å‹åç§°éƒ½ä¼šè·¯ç”±åˆ° K2-Think
- âœ… **OpenAI å®Œç¾æ›¿ä»£**ï¼šå¯ä½œä¸º OpenAI API çš„ç›´æ¥æ›¿ä»£å“
- âœ… **é›¶é…ç½®**ï¼šæ— éœ€ç®¡ç† API å¯†é’¥æˆ–æ¨¡å‹åç§°

### å¿«é€Ÿç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-any",  # ä»»ä½•å¯†é’¥éƒ½å¯ä»¥ï¼
    base_url="http://localhost:7000/v1"
)

result = client.chat.completions.create(
    model="gpt-5",  # ä»»ä½•æ¨¡å‹åéƒ½å¯ä»¥ï¼
    messages=[{"role": "user", "content": "ä½ å¥½ï¼"}]
)

print(result.choices[0].message.content)
```

### ğŸš€ ä¸€é”®éƒ¨ç½²

```bash
curl -fsSL https://raw.githubusercontent.com/Zeeeepa/k2think2api3/main/scripts/all.sh | bash
```

### å¿«é€Ÿå‘½ä»¤

```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f server.log

# åœæ­¢æœåŠ¡å™¨
kill $(cat .server.pid)

# é‡å¯æœåŠ¡å™¨
bash scripts/start.sh

# æµ‹è¯• API
bash scripts/send_request.sh
```

</details>

---

## Support & Contributing

### Getting Help

- ğŸ“– [GitHub Repository](https://github.com/Zeeeepa/k2think2api3)
- ğŸ› [Report Issues](https://github.com/Zeeeepa/k2think2api3/issues)
- ğŸ’¬ Check server logs: `tail -f server.log`
- ğŸ” Verify configuration: `cat .env`

### Pro Tips

1. **Keep credentials secure**: Never commit `accounts.txt` or `.env`
2. **Monitor token usage**: Check `/admin/tokens/stats` regularly
3. **Use environment variables**: Easier than hardcoding
4. **Always use venv**: For consistent dependencies
5. **Logs are your friend**: Check `server.log` when troubleshooting
6. **Health checks**: Monitor `/health` in production

---

## License

This project is provided as-is for educational and development purposes.

---

**Happy coding! ğŸ‰**

---

*Last updated: 2025-01-15*  
*Version: 2.1.0 - Open Proxy Mode*
