from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.responses import StreamingResponse, JSONResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Union, AsyncGenerator
import httpx
import json
import asyncio
import time
import os
import logging
import re
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
VALID_API_KEY = os.getenv("VALID_API_KEY", "sk-k2think")
K2THINK_API_URL = os.getenv("K2THINK_API_URL", "https://www.k2think.ai/api/chat/completions")
K2THINK_TOKEN = os.getenv("K2THINK_TOKEN")
OUTPUT_THINKING = os.getenv("OUTPUT_THINKING", "true").lower() == "true"
TOOL_SUPPORT = os.getenv("TOOL_SUPPORT", "true").lower() == "true"
SCAN_LIMIT = int(os.getenv("SCAN_LIMIT", "200000"))

# é«˜çº§é…ç½®
REQUEST_TIMEOUT = float(os.getenv("REQUEST_TIMEOUT", "60"))
MAX_KEEPALIVE_CONNECTIONS = int(os.getenv("MAX_KEEPALIVE_CONNECTIONS", "20"))
MAX_CONNECTIONS = int(os.getenv("MAX_CONNECTIONS", "100"))
DEBUG_LOGGING = os.getenv("DEBUG_LOGGING", "false").lower() == "true"
STREAM_DELAY = float(os.getenv("STREAM_DELAY", "0.05"))
STREAM_CHUNK_SIZE = int(os.getenv("STREAM_CHUNK_SIZE", "50"))
ENABLE_ACCESS_LOG = os.getenv("ENABLE_ACCESS_LOG", "true").lower() == "true"
CORS_ORIGINS = os.getenv("CORS_ORIGINS", "*").split(",") if os.getenv("CORS_ORIGINS", "*") != "*" else ["*"]

# è®¾ç½®æ—¥å¿—
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
if LOG_LEVEL == "DEBUG":
    logging.basicConfig(level=logging.DEBUG)
elif LOG_LEVEL == "WARNING":
    logging.basicConfig(level=logging.WARNING)
elif LOG_LEVEL == "ERROR":
    logging.basicConfig(level=logging.ERROR)
else:
    logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(__name__)

# æ•°æ®æ¨¡å‹
class ContentPart(BaseModel):
    """Content part model for OpenAI's new content format"""
    type: str
    text: Optional[str] = None

class Message(BaseModel):
    role: str
    content: Optional[Union[str, List[ContentPart]]] = None
    tool_calls: Optional[List[Dict]] = None

class ChatCompletionRequest(BaseModel):
    model: str = "MBZUAI-IFM/K2-Think"
    messages: List[Message]
    stream: bool = False
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    stop: Optional[Union[str, List[str]]] = None
    tools: Optional[List[Dict]] = None
    tool_choice: Optional[Union[str, Dict]] = None

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str
    permission: List[Dict] = []
    root: str
    parent: Optional[str] = None

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]

# HTTPå®¢æˆ·ç«¯å·¥å‚å‡½æ•°
def create_http_client() -> httpx.AsyncClient:
    """åˆ›å»ºHTTPå®¢æˆ·ç«¯"""
    base_kwargs = {
        "timeout": httpx.Timeout(timeout=None, connect=10.0),
        "limits": httpx.Limits(
            max_keepalive_connections=MAX_KEEPALIVE_CONNECTIONS, 
            max_connections=MAX_CONNECTIONS
        ),
        "follow_redirects": True
    }
    
    try:
        return httpx.AsyncClient(**base_kwargs)
    except Exception as e:
        logger.error(f"åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: {e}")
        raise e

# å…¨å±€HTTPå®¢æˆ·ç«¯ç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    yield

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="K2Think API Proxy", lifespan=lifespan)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def validate_api_key(authorization: str) -> bool:
    """éªŒè¯APIå¯†é’¥"""
    if not authorization or not authorization.startswith("Bearer "):
        return False
    api_key = authorization[7:]  # ç§»é™¤ "Bearer " å‰ç¼€
    return api_key == VALID_API_KEY

def generate_session_id() -> str:
    """ç”Ÿæˆä¼šè¯ID"""
    import uuid
    return str(uuid.uuid4())

def generate_chat_id() -> str:
    """ç”ŸæˆèŠå¤©ID"""
    import uuid
    return str(uuid.uuid4())

def get_current_datetime_info():
    """è·å–å½“å‰æ—¶é—´ä¿¡æ¯"""
    from datetime import datetime
    import pytz
    
    # è®¾ç½®æ—¶åŒºä¸ºä¸Šæµ·
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    
    return {
        "{{USER_NAME}}": "User",
        "{{USER_LOCATION}}": "Unknown",
        "{{CURRENT_DATETIME}}": now.strftime("%Y-%m-%d %H:%M:%S"),
        "{{CURRENT_DATE}}": now.strftime("%Y-%m-%d"),
        "{{CURRENT_TIME}}": now.strftime("%H:%M:%S"),
        "{{CURRENT_WEEKDAY}}": now.strftime("%A"),
        "{{CURRENT_TIMEZONE}}": "Asia/Shanghai",
        "{{USER_LANGUAGE}}": "en-US"
    }

def extract_answer_content(full_content: str) -> str:
    """åˆ é™¤ç¬¬ä¸€ä¸ª<answer>æ ‡ç­¾å’Œæœ€åä¸€ä¸ª</answer>æ ‡ç­¾ï¼Œä¿ç•™å†…å®¹"""
    if not full_content:
        return full_content
    if OUTPUT_THINKING:
        # åˆ é™¤ç¬¬ä¸€ä¸ª<answer>
        answer_start = full_content.find('<answer>')
        if answer_start != -1:
            full_content = full_content[:answer_start] + full_content[answer_start + 8:]

        # åˆ é™¤æœ€åä¸€ä¸ª</answer>
        answer_end = full_content.rfind('</answer>')
        if answer_end != -1:
            full_content = full_content[:answer_end] + full_content[answer_end + 9:]

        return full_content.strip()
    else:
        # åˆ é™¤<think>éƒ¨åˆ†ï¼ˆåŒ…æ‹¬æ ‡ç­¾ï¼‰
        think_start = full_content.find('<think>')
        think_end = full_content.find('</think>')
        if think_start != -1 and think_end != -1:
            full_content = full_content[:think_start] + full_content[think_end + 8:]
        
        # åˆ é™¤<answer>æ ‡ç­¾åŠå…¶å†…å®¹ä¹‹å¤–çš„éƒ¨åˆ†
        answer_start = full_content.find('<answer>')
        answer_end = full_content.rfind('</answer>')
        if answer_start != -1 and answer_end != -1:
            content = full_content[answer_start + 8:answer_end]
            return content.strip()

        return full_content.strip()

def content_to_string(content) -> str:
    """Convert content from various formats to string"""
    if content is None:
        return ""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = []
        for p in content:
            if hasattr(p, 'text'):  # ContentPart object
                parts.append(getattr(p, 'text', ''))
            elif isinstance(p, dict) and p.get("type") == "text":
                parts.append(p.get("text", ""))
            elif isinstance(p, str):
                parts.append(p)
            else:
                # å¤„ç†å…¶ä»–ç±»å‹çš„å¯¹è±¡
                try:
                    if hasattr(p, '__dict__'):
                        # å¦‚æœæ˜¯å¯¹è±¡ï¼Œå°è¯•è·å–textå±æ€§æˆ–è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        parts.append(str(getattr(p, 'text', str(p))))
                    else:
                        parts.append(str(p))
                except:
                    continue
        return " ".join(parts)
    # å¤„ç†å…¶ä»–ç±»å‹
    try:
        return str(content)
    except:
        return ""

def generate_tool_prompt(tools: List[Dict]) -> str:
    """Generate concise tool injection prompt"""
    if not tools:
        return ""

    tool_definitions = []
    for tool in tools:
        if tool.get("type") != "function":
            continue

        function_spec = tool.get("function", {}) or {}
        function_name = function_spec.get("name", "unknown")
        function_description = function_spec.get("description", "")
        parameters = function_spec.get("parameters", {}) or {}

        # Create concise tool definition
        tool_info = f"{function_name}: {function_description}"
        
        # Add simplified parameter info
        parameter_properties = parameters.get("properties", {}) or {}
        required_parameters = set(parameters.get("required", []) or [])

        if parameter_properties:
            param_list = []
            for param_name, param_details in parameter_properties.items():
                param_desc = (param_details or {}).get("description", "")
                is_required = param_name in required_parameters
                param_list.append(f"{param_name}{'*' if is_required else ''}: {param_desc}")
            tool_info += f" Parameters: {', '.join(param_list)}"

        tool_definitions.append(tool_info)

    if not tool_definitions:
        return ""

    # Build concise tool prompt
    prompt_template = (
        f"\n\nAvailable tools: {'; '.join(tool_definitions)}. "
        "To use a tool, respond with JSON: "
        '{"tool_calls":[{"id":"call_xxx","type":"function","function":{"name":"tool_name","arguments":"{\\"param\\":\\"value\\"}"}}]}'
    )

    return prompt_template

def process_messages_with_tools(messages: List[Dict], tools: Optional[List[Dict]] = None, tool_choice: Optional[Union[str, Dict]] = None) -> List[Dict]:
    """Process messages and inject tool prompts"""
    if not tools or not TOOL_SUPPORT or (tool_choice == "none"):
        # å¦‚æœæ²¡æœ‰å·¥å…·æˆ–ç¦ç”¨å·¥å…·ï¼Œç›´æ¥è¿”å›åŸæ¶ˆæ¯
        return [dict(m) for m in messages]
    
    tools_prompt = generate_tool_prompt(tools)
    
    # é™åˆ¶å·¥å…·æç¤ºé•¿åº¦ï¼Œé¿å…è¿‡é•¿å¯¼è‡´ä¸Šæ¸¸APIæ‹’ç»
    if len(tools_prompt) > 1000:
        logger.warning(f"å·¥å…·æç¤ºè¿‡é•¿ ({len(tools_prompt)} å­—ç¬¦)ï¼Œå°†æˆªæ–­")
        tools_prompt = tools_prompt[:1000] + "..."
    
    processed = []
    has_system = any(m.get("role") == "system" for m in messages)

    if has_system:
        # å¦‚æœå·²æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåœ¨ç¬¬ä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ å·¥å…·æç¤º
        for m in messages:
            if m.get("role") == "system":
                mm = dict(m)
                content = content_to_string(mm.get("content", ""))
                # ç¡®ä¿ç³»ç»Ÿæ¶ˆæ¯ä¸ä¼šè¿‡é•¿
                new_content = content + tools_prompt
                if len(new_content) > 2000:
                    logger.warning(f"ç³»ç»Ÿæ¶ˆæ¯è¿‡é•¿ ({len(new_content)} å­—ç¬¦)ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
                    mm["content"] = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚" + tools_prompt
                else:
                    mm["content"] = new_content
                processed.append(mm)
                # åªåœ¨ç¬¬ä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ å·¥å…·æç¤º
                tools_prompt = ""
            else:
                processed.append(dict(m))
    else:
        # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ªï¼Œä½†åªæœ‰å½“ç¡®å®éœ€è¦å·¥å…·æ—¶
        if tools_prompt.strip():
            processed = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚" + tools_prompt}]
            processed.extend([dict(m) for m in messages])
        else:
            processed = [dict(m) for m in messages]

    # Add simplified tool choice hints
    if tool_choice == "required":
        if processed and processed[-1].get("role") == "user":
            last = processed[-1]
            content = content_to_string(last.get("content", ""))
            last["content"] = content + "\nè¯·ä½¿ç”¨å·¥å…·æ¥å¤„ç†è¿™ä¸ªè¯·æ±‚ã€‚"
    elif isinstance(tool_choice, dict) and tool_choice.get("type") == "function":
        fname = (tool_choice.get("function") or {}).get("name")
        if fname and processed and processed[-1].get("role") == "user":
            last = processed[-1]
            content = content_to_string(last.get("content", ""))
            last["content"] = content + f"\nè¯·ä½¿ç”¨ {fname} å·¥å…·ã€‚"

    # Handle tool/function messages
    final_msgs = []
    for m in processed:
        role = m.get("role")
        if role in ("tool", "function"):
            tool_name = m.get("name", "unknown")
            tool_content = content_to_string(m.get("content", ""))
            if isinstance(tool_content, dict):
                tool_content = json.dumps(tool_content, ensure_ascii=False)

            # ç®€åŒ–å·¥å…·ç»“æœæ¶ˆæ¯
            content = f"å·¥å…· {tool_name} ç»“æœ: {tool_content}"
            if not content.strip():
                content = f"å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆ"

            final_msgs.append({
                "role": "assistant",
                "content": content,
            })
        else:
            # For regular messages, ensure content is string format
            final_msg = dict(m)
            content = content_to_string(final_msg.get("content", ""))
            final_msg["content"] = content
            final_msgs.append(final_msg)

    return final_msgs

# Tool Extraction Patterns
TOOL_CALL_FENCE_PATTERN = re.compile(r"```json\s*(\{.*?\})\s*```", re.DOTALL)
FUNCTION_CALL_PATTERN = re.compile(r"è°ƒç”¨å‡½æ•°\s*[ï¼š:]\s*([\w\-\.]+)\s*(?:å‚æ•°|arguments)[ï¼š:]\s*(\{.*?\})", re.DOTALL)

def extract_tool_invocations(text: str) -> Optional[List[Dict]]:
    """Extract tool invocations from response text"""
    if not text:
        return None

    # Limit scan size for performance
    scannable_text = text[:SCAN_LIMIT]

    # Attempt 1: Extract from JSON code blocks
    json_blocks = TOOL_CALL_FENCE_PATTERN.findall(scannable_text)
    for json_block in json_blocks:
        try:
            parsed_data = json.loads(json_block)
            tool_calls = parsed_data.get("tool_calls")
            if tool_calls and isinstance(tool_calls, list):
                # Ensure arguments field is a string
                for tc in tool_calls:
                    if "function" in tc:
                        func = tc["function"]
                        if "arguments" in func:
                            if isinstance(func["arguments"], dict):
                                # Convert dict to JSON string
                                func["arguments"] = json.dumps(func["arguments"], ensure_ascii=False)
                            elif not isinstance(func["arguments"], str):
                                func["arguments"] = json.dumps(func["arguments"], ensure_ascii=False)
                return tool_calls
        except (json.JSONDecodeError, AttributeError):
            continue

    # Attempt 2: Extract inline JSON objects using bracket balance method
    i = 0
    while i < len(scannable_text):
        if scannable_text[i] == '{':
            # å°è¯•æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·
            brace_count = 1
            j = i + 1
            in_string = False
            escape_next = False
            
            while j < len(scannable_text) and brace_count > 0:
                if escape_next:
                    escape_next = False
                elif scannable_text[j] == '\\':
                    escape_next = True
                elif scannable_text[j] == '"' and not escape_next:
                    in_string = not in_string
                elif not in_string:
                    if scannable_text[j] == '{':
                        brace_count += 1
                    elif scannable_text[j] == '}':
                        brace_count -= 1
                j += 1
            
            if brace_count == 0:
                # æ‰¾åˆ°äº†å®Œæ•´çš„ JSON å¯¹è±¡
                json_str = scannable_text[i:j]
                try:
                    parsed_data = json.loads(json_str)
                    tool_calls = parsed_data.get("tool_calls")
                    if tool_calls and isinstance(tool_calls, list):
                        # Ensure arguments field is a string
                        for tc in tool_calls:
                            if "function" in tc:
                                func = tc["function"]
                                if "arguments" in func:
                                    if isinstance(func["arguments"], dict):
                                        # Convert dict to JSON string
                                        func["arguments"] = json.dumps(func["arguments"], ensure_ascii=False)
                                    elif not isinstance(func["arguments"], str):
                                        func["arguments"] = json.dumps(func["arguments"], ensure_ascii=False)
                        return tool_calls
                except (json.JSONDecodeError, AttributeError):
                    pass
            
            i += 1
        else:
            i += 1

    # Attempt 3: Parse natural language function calls
    natural_lang_match = FUNCTION_CALL_PATTERN.search(scannable_text)
    if natural_lang_match:
        function_name = natural_lang_match.group(1).strip()
        arguments_str = natural_lang_match.group(2).strip()
        try:
            # Validate JSON format
            json.loads(arguments_str)
            return [
                {
                    "id": f"call_{int(time.time() * 1000000)}",
                    "type": "function",
                    "function": {"name": function_name, "arguments": arguments_str},
                }
            ]
        except json.JSONDecodeError:
            return None

    return None

def remove_tool_json_content(text: str) -> str:
    """Remove tool JSON content from response text - using bracket balance method"""
    
    def remove_tool_call_block(match: re.Match) -> str:
        json_content = match.group(1)
        try:
            parsed_data = json.loads(json_content)
            if "tool_calls" in parsed_data:
                return ""
        except (json.JSONDecodeError, AttributeError):
            pass
        return match.group(0)
    
    # Step 1: Remove fenced tool JSON blocks
    cleaned_text = TOOL_CALL_FENCE_PATTERN.sub(remove_tool_call_block, text)
    
    # Step 2: Remove inline tool JSON - ä½¿ç”¨åŸºäºæ‹¬å·å¹³è¡¡çš„æ™ºèƒ½æ–¹æ³•
    result = []
    i = 0
    while i < len(cleaned_text):
        if cleaned_text[i] == '{':
            # å°è¯•æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·
            brace_count = 1
            j = i + 1
            in_string = False
            escape_next = False
            
            while j < len(cleaned_text) and brace_count > 0:
                if escape_next:
                    escape_next = False
                elif cleaned_text[j] == '\\':
                    escape_next = True
                elif cleaned_text[j] == '"' and not escape_next:
                    in_string = not in_string
                elif not in_string:
                    if cleaned_text[j] == '{':
                        brace_count += 1
                    elif cleaned_text[j] == '}':
                        brace_count -= 1
                j += 1
            
            if brace_count == 0:
                # æ‰¾åˆ°äº†å®Œæ•´çš„ JSON å¯¹è±¡
                json_str = cleaned_text[i:j]
                try:
                    parsed = json.loads(json_str)
                    if "tool_calls" in parsed:
                        # è¿™æ˜¯ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼Œè·³è¿‡å®ƒ
                        i = j
                        continue
                except:
                    pass
            
            # ä¸æ˜¯å·¥å…·è°ƒç”¨æˆ–æ— æ³•è§£æï¼Œä¿ç•™è¿™ä¸ªå­—ç¬¦
            result.append(cleaned_text[i])
            i += 1
        else:
            result.append(cleaned_text[i])
            i += 1
    
    return ''.join(result).strip()

async def make_request(method: str, url: str, headers: dict, json_data: dict = None, 
                      stream: bool = False) -> httpx.Response:
    """å‘é€HTTPè¯·æ±‚"""
    client = None
    
    try:
        client = create_http_client()
        
        if stream:
            # æµå¼è¯·æ±‚è¿”å›context manager
            return client.stream(method, url, headers=headers, json=json_data, timeout=None)
        else:
            response = await client.request(method, url, headers=headers, json=json_data, timeout=REQUEST_TIMEOUT)
            
            # è¯¦ç»†è®°å½•é200å“åº”
            if response.status_code != 200:
                logger.error(f"ä¸Šæ¸¸APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                logger.error(f"å“åº”å¤´: {dict(response.headers)}")
                try:
                    error_body = response.text
                    logger.error(f"é”™è¯¯å“åº”ä½“: {error_body}")
                except:
                    logger.error("æ— æ³•è¯»å–é”™è¯¯å“åº”ä½“")
            
            response.raise_for_status()
            return response
            
    except httpx.HTTPStatusError as e:
        logger.error(f"HTTPçŠ¶æ€é”™è¯¯: {e.response.status_code} - {e.response.text}")
        if client and not stream:
            await client.aclose()
        raise e
    except Exception as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
        if client and not stream:
            await client.aclose()
        raise e

@app.get("/")
async def homepage():
    """é¦–é¡µ - è¿”å›æœåŠ¡çŠ¶æ€"""
    return JSONResponse(content={
        "status": "success",
        "message": "K2Think API Proxy is running",
        "service": "K2Think API Gateway", 
        "model": "MBZUAI-IFM/K2-Think",
        "version": "1.0.0",
        "endpoints": {
            "chat": "/v1/chat/completions",
            "models": "/v1/models"
        }
    })

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse(content={
        "status": "healthy",
        "timestamp": int(time.time())
    })

@app.get("/favicon.ico")
async def favicon():
    """è¿”å›favicon"""
    return Response(content="", media_type="image/x-icon")

@app.get("/v1/models")
async def get_models() -> ModelsResponse:
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    model_info = ModelInfo(
        id="MBZUAI-IFM/K2-Think",
        created=int(time.time()),
        owned_by="MBZUAI",
        root="mbzuai-k2-think-2508"
    )
    return ModelsResponse(data=[model_info])


@app.get("/favicon.ico")
async def favicon():
    """è¿”å›favicon"""
    return Response(content="", media_type="image/x-icon")

@app.get("/v1/models")
async def get_models() -> ModelsResponse:
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    model_info = ModelInfo(
        id="MBZUAI-IFM/K2-Think",
        created=int(time.time()),
        owned_by="MBZUAI",
        root="mbzuai-k2-think-2508"
    )
    return ModelsResponse(data=[model_info])

async def process_non_stream_response(k2think_payload: dict, headers: dict) -> tuple[str, dict]:
    """å¤„ç†éæµå¼å“åº”"""
    try:
        response = await make_request(
            "POST", 
            K2THINK_API_URL, 
            headers, 
            k2think_payload, 
            stream=False
        )
        
        # K2Think éæµå¼è¯·æ±‚è¿”å›æ ‡å‡†JSONæ ¼å¼
        result = response.json()
        
        # æå–å†…å®¹
        full_content = ""
        if result.get('choices') and len(result['choices']) > 0:
            choice = result['choices'][0]
            if choice.get('message') and choice['message'].get('content'):
                raw_content = choice['message']['content']
                # æå–<answer>æ ‡ç­¾ä¸­çš„å†…å®¹ï¼Œå»é™¤æ ‡ç­¾
                full_content = extract_answer_content(raw_content)
        
        # æå–tokenä¿¡æ¯
        token_info = result.get('usage', {
            "prompt_tokens": 0, 
            "completion_tokens": 0, 
            "total_tokens": 0
        })
        
        await response.aclose()
        return full_content, token_info
                    
    except Exception as e:
        logger.error(f"å¤„ç†éæµå¼å“åº”é”™è¯¯: {e}")
        raise

async def process_stream_response(k2think_payload: dict, headers: dict) -> AsyncGenerator[str, None]:
    """å¤„ç†æµå¼å“åº” - ä½¿ç”¨æ¨¡æ‹Ÿæµå¼è¾“å‡º"""
    try:
        # å°†æµå¼è¯·æ±‚è½¬æ¢ä¸ºéæµå¼è¯·æ±‚
        k2think_payload_copy = k2think_payload.copy()
        k2think_payload_copy["stream"] = False
        
        # ä¿®æ”¹headersä¸ºéæµå¼
        headers_copy = headers.copy()
        headers_copy["accept"] = "application/json"
        
        # è·å–å®Œæ•´å“åº”
        full_content, token_info = await process_non_stream_response(k2think_payload_copy, headers_copy)
        
        if not full_content:
            yield "data: [DONE]\n\n"
            return
        
        # å¼€å§‹æµå¼è¾“å‡º - å‘é€å¼€å§‹chunk
        start_chunk = {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "MBZUAI-IFM/K2-Think",
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": ""
                },
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(start_chunk)}\n\n"
        
        # æ¨¡æ‹Ÿæµå¼è¾“å‡º - æŒ‰å­—ç¬¦åˆ†å—å‘é€
        
        chunk_size = STREAM_CHUNK_SIZE  # æ¯æ¬¡å‘é€nä¸ªå­—ç¬¦
        
        for i in range(0, len(full_content), chunk_size):
            chunk_content = full_content[i:i + chunk_size]
            
            chunk = {
                "id": f"chatcmpl-{int(time.time() * 1000)}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": "MBZUAI-IFM/K2-Think",
                "choices": [{
                    "index": 0,
                    "delta": {
                        "content": chunk_content
                    },
                    "finish_reason": None
                }]
            }
            
            yield f"data: {json.dumps(chunk)}\n\n"
            # æ·»åŠ å°å»¶è¿Ÿæ¨¡æ‹ŸçœŸå®æµå¼æ•ˆæœ
            await asyncio.sleep(STREAM_DELAY)
        
        # å‘é€ç»“æŸchunk
        end_chunk = {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "MBZUAI-IFM/K2-Think",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(end_chunk)}\n\n"
        yield "data: [DONE]\n\n"
                
    except Exception as e:
        logger.error(f"æµå¼è¯·æ±‚å¤±è´¥: {e}")
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_chunk = {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "MBZUAI-IFM/K2-Think",
            "choices": [{
                "index": 0,
                "delta": {
                    "content": f"Error: {str(e)}"
                },
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"

async def process_stream_response_with_tools(k2think_payload: dict, headers: dict, has_tools: bool = False) -> AsyncGenerator[str, None]:
    """å¤„ç†æµå¼å“åº” - æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œä¼˜åŒ–æ€§èƒ½"""
    try:
        # å‘é€å¼€å§‹chunk
        start_chunk = {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "MBZUAI-IFM/K2-Think",
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": ""
                },
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(start_chunk)}\n\n"
        
        # ä¼˜åŒ–çš„æ¨¡æ‹Ÿæµå¼è¾“å‡º - ç«‹å³å¼€å§‹è·å–å“åº”å¹¶æµå¼å‘é€
        k2think_payload_copy = k2think_payload.copy()
        k2think_payload_copy["stream"] = False
        
        headers_copy = headers.copy()
        headers_copy["accept"] = "application/json"
        
        # è·å–å®Œæ•´å“åº”
        full_content, token_info = await process_non_stream_response(k2think_payload_copy, headers_copy)
        
        if not full_content:
            yield "data: [DONE]\n\n"
            return
        
        # Handle tool calls for streaming
        finish_reason = "stop"
        if has_tools:
            tool_calls = extract_tool_invocations(full_content)
            if tool_calls:
                # Send tool calls with proper format
                for i, tc in enumerate(tool_calls):
                    tool_call_delta = {
                        "index": i,
                        "id": tc.get("id"),
                        "type": tc.get("type", "function"),
                        "function": tc.get("function", {}),
                    }
                    
                    tool_chunk = {
                        "id": f"chatcmpl-{int(time.time() * 1000)}",
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": "MBZUAI-IFM/K2-Think",
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "tool_calls": [tool_call_delta]
                            },
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(tool_chunk)}\n\n"
                
                finish_reason = "tool_calls"
            else:
                # Send regular content with true streaming feel
                trimmed_content = remove_tool_json_content(full_content)
                if trimmed_content:
                    # å¿«é€Ÿæµå¼è¾“å‡º - åˆç†çš„å—å¤§å°
                    chunk_size = STREAM_CHUNK_SIZE  # æ¯æ¬¡å‘é€nä¸ªå­—ç¬¦ï¼Œä¿æŒæµå¼æ„Ÿè§‰ä½†é€Ÿåº¦å¿«
                    
                    for i in range(0, len(trimmed_content), chunk_size):
                        chunk_content = trimmed_content[i:i + chunk_size]
                        
                        chunk = {
                            "id": f"chatcmpl-{int(time.time() * 1000)}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": "MBZUAI-IFM/K2-Think",
                            "choices": [{
                                "index": 0,
                                "delta": {
                                    "content": chunk_content
                                },
                                "finish_reason": None
                            }]
                        }
                        
                        yield f"data: {json.dumps(chunk)}\n\n"
                        # æ·»åŠ æå°å»¶è¿Ÿç¡®ä¿å—åˆ†åˆ«å‘é€
                        await asyncio.sleep(STREAM_DELAY/2)  # æ¯«ç§’å»¶è¿Ÿ
        else:
            # No tools - send regular content with fast streaming
            chunk_size = STREAM_CHUNK_SIZE  # æ¯æ¬¡å‘é€nä¸ªå­—ç¬¦ï¼Œä¿æŒæµå¼æ„Ÿè§‰ä½†é€Ÿåº¦å¿«
            
            for i in range(0, len(full_content), chunk_size):
                chunk_content = full_content[i:i + chunk_size]
                
                chunk = {
                    "id": f"chatcmpl-{int(time.time() * 1000)}",
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": "MBZUAI-IFM/K2-Think",
                    "choices": [{
                        "index": 0,
                        "delta": {
                            "content": chunk_content
                        },
                        "finish_reason": None
                    }]
                }
                
                yield f"data: {json.dumps(chunk)}\n\n"
                # æ·»åŠ æå°å»¶è¿Ÿç¡®ä¿å—åˆ†åˆ«å‘é€
                await asyncio.sleep(STREAM_DELAY/2)  # æ¯«ç§’å»¶è¿Ÿ
        
        # å‘é€ç»“æŸchunk
        end_chunk = {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "MBZUAI-IFM/K2-Think",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": finish_reason
            }]
        }
        yield f"data: {json.dumps(end_chunk)}\n\n"
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"æµå¼å“åº”å¤„ç†é”™è¯¯: {e}")
        error_chunk = {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "MBZUAI-IFM/K2-Think",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "error"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
        yield "data: [DONE]\n\n"

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest, auth_request: Request):
    """å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚"""
    # éªŒè¯APIå¯†é’¥
    authorization = auth_request.headers.get("Authorization", "")
    if not validate_api_key(authorization):
        raise HTTPException(
            status_code=401,
            detail={
                "error": {
                    "message": "Invalid API key provided",
                    "type": "authentication_error"
                }
            }
        )
    
    try:
        # Process messages with tools - ç¡®ä¿å†…å®¹è¢«æ­£ç¡®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        raw_messages = []
        for msg in request.messages:
            try:
                content = content_to_string(msg.content)
                raw_messages.append({
                    "role": msg.role, 
                    "content": content, 
                    "tool_calls": msg.tool_calls
                })
            except Exception as e:
                logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}, æ¶ˆæ¯: {msg}")
                # ä½¿ç”¨é»˜è®¤å€¼
                raw_messages.append({
                    "role": msg.role, 
                    "content": str(msg.content) if msg.content else "", 
                    "tool_calls": msg.tool_calls
                })
        
        # Check if tools are enabled and present
        has_tools = (TOOL_SUPPORT and 
                    request.tools and 
                    len(request.tools) > 0 and 
                    request.tool_choice != "none")
        
        logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨çŠ¶æ€: has_tools={has_tools}, tools_count={len(request.tools) if request.tools else 0}")
        logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°çš„åŸå§‹æ¶ˆæ¯æ•°: {len(raw_messages)}")
        
        # è®°å½•åŸå§‹æ¶ˆæ¯çš„è§’è‰²åˆ†å¸ƒ
        role_count = {}
        for msg in raw_messages:
            role = msg.get("role", "unknown")
            role_count[role] = role_count.get(role, 0) + 1
        logger.info(f"ğŸ“Š åŸå§‹æ¶ˆæ¯è§’è‰²åˆ†å¸ƒ: {role_count}")
        
        if has_tools:
            processed_messages = process_messages_with_tools(
                raw_messages,
                request.tools,
                request.tool_choice
            )
            logger.info(f"ğŸ”„ æ¶ˆæ¯å¤„ç†å®Œæˆï¼ŒåŸå§‹æ¶ˆæ¯æ•°: {len(raw_messages)}, å¤„ç†åæ¶ˆæ¯æ•°: {len(processed_messages)}")
            
            # è®°å½•å¤„ç†åæ¶ˆæ¯çš„è§’è‰²åˆ†å¸ƒ
            processed_role_count = {}
            for msg in processed_messages:
                role = msg.get("role", "unknown")
                processed_role_count[role] = processed_role_count.get(role, 0) + 1
            logger.info(f"ğŸ“Š å¤„ç†åæ¶ˆæ¯è§’è‰²åˆ†å¸ƒ: {processed_role_count}")
        else:
            processed_messages = raw_messages
            logger.info("â­ï¸  æ— å·¥å…·è°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ¶ˆæ¯")
        
        # æ„å»º K2Think æ ¼å¼çš„è¯·æ±‚ä½“ - ç¡®ä¿æ‰€æœ‰å†…å®¹å¯JSONåºåˆ—åŒ–
        k2think_messages = []
        for msg in processed_messages:
            try:
                # ç¡®ä¿æ¶ˆæ¯å†…å®¹æ˜¯å­—ç¬¦ä¸²
                content = content_to_string(msg.get("content", ""))
                k2think_messages.append({
                    "role": msg["role"], 
                    "content": content
                })
            except Exception as e:
                logger.error(f"æ„å»ºK2Thinkæ¶ˆæ¯æ—¶å‡ºé”™: {e}, æ¶ˆæ¯: {msg}")
                # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
                k2think_messages.append({
                    "role": msg.get("role", "user"), 
                    "content": str(msg.get("content", ""))
                })
        
        k2think_payload = {
            "stream": request.stream,
            "model": "MBZUAI-IFM/K2-Think",
            "messages": k2think_messages,
            "params": {},
            "tool_servers": [],
            "features": {
                "image_generation": False,
                "code_interpreter": False,
                "web_search": False
            },
            "variables": get_current_datetime_info(),
            "model_item": {
                "id": "MBZUAI-IFM/K2-Think",
                "object": "model",
                "owned_by": "MBZUAI",
                "root": "mbzuai-k2-think-2508",
                "parent": None,
                "status": "active",
                "connection_type": "external",
                "name": "MBZUAI-IFM/K2-Think"
            },
            "background_tasks": {
                "title_generation": True,
                "tags_generation": True
            },
            "chat_id": generate_chat_id(),
            "id": generate_session_id(),
            "session_id": generate_session_id()
        }
        
        # éªŒè¯JSONåºåˆ—åŒ–å¹¶è®°å½•å‘é€åˆ°ä¸Šæ¸¸çš„è¯·æ±‚
        try:
            # æµ‹è¯•JSONåºåˆ—åŒ–
            json.dumps(k2think_payload, ensure_ascii=False)
            logger.info(f"âœ… K2Thinkè¯·æ±‚ä½“JSONåºåˆ—åŒ–éªŒè¯é€šè¿‡")
        except Exception as e:
            logger.error(f"âŒ K2Thinkè¯·æ±‚ä½“JSONåºåˆ—åŒ–å¤±è´¥: {e}")
            # å°è¯•ä¿®å¤åºåˆ—åŒ–é—®é¢˜
            try:
                k2think_payload = json.loads(json.dumps(k2think_payload, default=str, ensure_ascii=False))
                logger.info("ğŸ”§ ä½¿ç”¨default=strä¿®å¤äº†åºåˆ—åŒ–é—®é¢˜")
            except Exception as fix_error:
                logger.error(f"æ— æ³•ä¿®å¤åºåˆ—åŒ–é—®é¢˜: {fix_error}")
                raise HTTPException(status_code=500, detail="è¯·æ±‚æ•°æ®åºåˆ—åŒ–å¤±è´¥")
        
        logger.info(f"å‘é€åˆ° K2Think çš„æ¶ˆæ¯æ•°é‡: {len(k2think_payload['messages'])}")
        if DEBUG_LOGGING or logger.level <= logging.DEBUG:
            for i, msg in enumerate(k2think_payload['messages']):
                content_preview = msg['content'][:200] + "..." if len(msg['content']) > 200 else msg['content']
                logger.debug(f"æ¶ˆæ¯ {i+1} ({msg['role']}): {content_preview}")
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "accept": "text/event-stream,application/json" if request.stream else "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {K2THINK_TOKEN}",
            "origin": "https://www.k2think.ai",
            "referer": "https://www.k2think.ai/c/" + k2think_payload["chat_id"],
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0"
        }
        
        if request.stream:
            # æµå¼å“åº”
            return StreamingResponse(
                process_stream_response_with_tools(k2think_payload, headers, has_tools),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )
        else:
            # éæµå¼å“åº”
            full_content, token_info = await process_non_stream_response(k2think_payload, headers)
            
            # Handle tool calls for non-streaming
            tool_calls = None
            finish_reason = "stop"
            message_content = full_content
            
            if has_tools:
                tool_calls = extract_tool_invocations(full_content)
                if tool_calls:
                    # Content must be null when tool_calls are present (OpenAI spec)
                    message_content = None
                    finish_reason = "tool_calls"
                    logger.info(f"æå–åˆ°å·¥å…·è°ƒç”¨: {json.dumps(tool_calls, ensure_ascii=False)}")
                else:
                    # Remove tool JSON from content
                    message_content = remove_tool_json_content(full_content)
                    if not message_content:
                        message_content = full_content  # ä¿ç•™åŸå†…å®¹å¦‚æœæ¸…ç†åä¸ºç©º
            
            openai_response = {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": "MBZUAI-IFM/K2-Think",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": message_content,
                        **({"tool_calls": tool_calls} if tool_calls else {})
                    },
                    "finish_reason": finish_reason
                }],
                "usage": token_info
            }
            
            return JSONResponse(content=openai_response)
                
    except httpx.HTTPStatusError as e:
        logger.error(f"HTTPé”™è¯¯: {e.response.status_code}")
        raise HTTPException(
            status_code=e.response.status_code,
            detail={
                "error": {
                    "message": f"ä¸Šæ¸¸æœåŠ¡é”™è¯¯: {e.response.status_code}",
                    "type": "upstream_error"
                }
            }
        )
    except httpx.TimeoutException:
        logger.error("è¯·æ±‚è¶…æ—¶")
        raise HTTPException(
            status_code=504,
            detail={
                "error": {
                    "message": "è¯·æ±‚è¶…æ—¶",
                    "type": "timeout_error"
                }
            }
        )
    except Exception as e:
        logger.error(f"APIè½¬å‘é”™è¯¯: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": {
                    "message": str(e),
                    "type": "api_error"
                }
            }
        )

@app.exception_handler(404)
async def not_found_handler(request: Request, exc):
    return JSONResponse(
        status_code=404,
        content={"error": "Not Found"}
    )

if __name__ == "__main__":
    import uvicorn
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8001"))
    
    # é…ç½®æ—¥å¿—çº§åˆ«
    log_level = "debug" if DEBUG_LOGGING else "info"
    
    uvicorn.run(
        app, 
        host=host, 
        port=port, 
        access_log=ENABLE_ACCESS_LOG,
        log_level=log_level
    )